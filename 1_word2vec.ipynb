{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use rpy2 for some operations be completed using R syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo pip3 install nltk\n",
    "# sudo pip3 install pandas\n",
    "# sudo pip3 install rpy2 && sudo pip3 install gensim\n",
    "# sudo pip3 install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: During startup - \n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: Warning messages:\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: 1: Setting LC_TIME failed, using \"C\" \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: 2: Setting LC_MONETARY failed, using \"C\" \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: 3: Setting LC_PAPER failed, using \"C\" \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/usr/local/lib/python3.5/dist-packages/rpy2/rinterface/__init__.py:145: RRuntimeWarning: 4: Setting LC_MEASUREMENT failed, using \"C\" \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects.packages import importr\n",
    "import rpy2.robjects.numpy2ri\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "# tidyr = importr('tidyverse')\n",
    "# magrittr = importr('magrittr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a single courpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rstring = \"\"\"\n",
    "#     function(){  \n",
    "    \n",
    "#     l = list.files(\"~/imdb/data/train/pos/\", full.names = T)\n",
    "\n",
    "#     t = data.frame()\n",
    "\n",
    "#     for (i in 1:length(l)){\n",
    "#       t = rbind(t, data.frame(path = l[i], \n",
    "#                      text = readLines(l[i])))\n",
    "#     }\n",
    "    \n",
    "#     t\n",
    "    \n",
    "#     }\n",
    "# \"\"\"\n",
    "\n",
    "# rfunc = robjects.r(rstring)\n",
    "# r_df = rfunc()\n",
    "# df = pandas2ri.ri2py(r_df)\n",
    "\n",
    "\n",
    "# df.to_csv(\"~/imdb/data/df_texts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"~/imdb/data/df_texts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8754ddac7a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Transform to lower case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[0-9]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Transform to lower case\n",
    "import re\n",
    "corpus = df[\"text\"].str.lower()\n",
    "corpus = re.sub('[0-9]+', '', corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Removing punctuation\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences = [tokenizer.tokenize(doc) for doc in corpus]\n",
    "\n",
    "# Stemming and Lemmatisation\n",
    "\n",
    "## stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "## lemmatisation\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "sentences = [[wordnet_lemmatizer.lemmatize(porter_stemmer.stem(token)) for token in sentence] for sentence in sentences]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters:\n",
    "\n",
    "- sentences - data\n",
    "- sg - algorirthms: 0 - CBOW, 1 - Skip-gram\n",
    "- size - vector size, dimensionality \n",
    "- window - windows size\n",
    "- min_count - threshold of word count to be in a vocabulary\n",
    "- max_vocab_size - keeping more-less frequent words\n",
    "- sample - downsampling of frequent words. Smaller value - less probability to keep it\n",
    "- alpha - learning rate\n",
    "- min_alpha - minimum learning rate\n",
    "- negative - negative examples # higher values are appropriate for a smaller corpus\n",
    "- iter -- number of iterations\n",
    "\n",
    "Practical difference between CBOW and Skip-gram is that the latter is better dealing with less frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 2442598 words, keeping 33760 word types\n",
      "INFO : collected 37617 word types from a corpus of 3060552 raw words and 12500 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=20 retains 7048 unique words (18% of original 37617, drops 30569)\n",
      "INFO : min_count=20 leaves 2945923 word corpus (96% of original 3060552, drops 114629)\n",
      "INFO : deleting the raw counts dictionary of 37617 items\n",
      "INFO : sample=0.001 downsamples 49 most-common words\n",
      "INFO : downsampling leaves estimated 2137631 word corpus (72.6% of prior 2945923)\n",
      "INFO : estimated required memory for 7048 words and 150 dimensions: 11981600 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 7048 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : PROGRESS: at 1.09% examples, 110354 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 2.45% examples, 126801 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 3.80% examples, 131370 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 5.11% examples, 131478 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 6.19% examples, 128788 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 7.16% examples, 124174 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 8.24% examples, 121888 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 9.44% examples, 122251 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 10.88% examples, 124793 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 12.04% examples, 124463 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 13.07% examples, 122347 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 14.14% examples, 121113 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 15.21% examples, 121137 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 16.54% examples, 122311 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 17.84% examples, 122413 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 18.88% examples, 121199 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 19.89% examples, 120127 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 20.94% examples, 119366 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 22.05% examples, 118121 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 23.04% examples, 117600 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 24.05% examples, 117143 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 25.07% examples, 116750 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : PROGRESS: at 26.08% examples, 115925 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 27.13% examples, 115320 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 28.21% examples, 115014 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 29.41% examples, 115534 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 30.80% examples, 116492 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 32.15% examples, 117386 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 33.37% examples, 117801 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 34.76% examples, 118530 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 35.91% examples, 118576 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 36.76% examples, 117866 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 37.82% examples, 117450 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 39.10% examples, 118101 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 40.49% examples, 118832 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 41.90% examples, 119395 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 43.17% examples, 119891 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 44.61% examples, 120435 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 45.94% examples, 121050 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 47.31% examples, 121466 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 48.59% examples, 121452 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 49.77% examples, 121516 words/s, in_qsize 5, out_qsize 2\n",
      "INFO : PROGRESS: at 51.25% examples, 122013 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : PROGRESS: at 52.59% examples, 122440 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 54.00% examples, 122863 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 55.21% examples, 123111 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 56.55% examples, 123498 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 57.82% examples, 123746 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 59.03% examples, 123923 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 60.43% examples, 124296 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 61.91% examples, 124752 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 63.04% examples, 124754 words/s, in_qsize 6, out_qsize 1\n",
      "INFO : PROGRESS: at 64.20% examples, 124547 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 65.44% examples, 124733 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 66.82% examples, 124890 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 68.27% examples, 125278 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 69.58% examples, 125570 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 70.93% examples, 125737 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 72.20% examples, 125844 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 73.61% examples, 126127 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 74.82% examples, 126210 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 76.16% examples, 126379 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 77.59% examples, 126602 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 78.62% examples, 126356 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 79.49% examples, 125833 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 80.90% examples, 126023 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 82.34% examples, 126310 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 83.58% examples, 126288 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 84.87% examples, 126533 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 86.15% examples, 126662 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 87.07% examples, 126126 words/s, in_qsize 5, out_qsize 2\n",
      "INFO : PROGRESS: at 88.59% examples, 126373 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 89.83% examples, 126392 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 91.25% examples, 126447 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 92.25% examples, 126177 words/s, in_qsize 7, out_qsize 1\n",
      "INFO : PROGRESS: at 93.30% examples, 125848 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 94.36% examples, 125543 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 95.33% examples, 125312 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 96.35% examples, 124952 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 97.44% examples, 124687 words/s, in_qsize 8, out_qsize 0\n",
      "INFO : PROGRESS: at 98.45% examples, 124431 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 99.43% examples, 124205 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : training on 15302760 raw words (10687967 effective words) took 86.0s, 124223 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences,\n",
    "                    sg = 1,\n",
    "                    window = 10,\n",
    "                    min_count = 20,\n",
    "                    seed = 6,\n",
    "                    workers = 4,\n",
    "                    size = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : saving Word2Vec object under ~/imdb/model.bin, separately None\n",
      "INFO : not storing attribute syn0norm\n",
      "INFO : not storing attribute cum_table\n",
      "INFO : saved ~/imdb/model.bin\n",
      "INFO : storing 7048x150 projection weights into ~/imdb/model.wv\n",
      "INFO : loading projection weights from ~/imdb/model.wv\n",
      "INFO : loaded (7048, 150) matrix from ~/imdb/model.wv\n"
     ]
    }
   ],
   "source": [
    "model.save('~/imdb/models/model.bin')\n",
    "model.wv.save_word2vec_format('~/imdb/models/model.wv')\n",
    "\n",
    "# save_word2vec_format\n",
    "# gensim.models.Word2Vec.load('~/imdb/models/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading projection weights from ~/imdb/models/model.wv\n",
      "INFO : loaded (7048, 150) matrix from ~/imdb/models/model.wv\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('~/imdb/models/model.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('actress', 0.6813883781433105),\n",
       " ('cast', 0.6340926289558411),\n",
       " ('stoltz', 0.5940563678741455),\n",
       " ('calib', 0.5839543342590332),\n",
       " ('ron', 0.583796501159668),\n",
       " ('versatil', 0.5788587927818298),\n",
       " ('phenomen', 0.5738711357116699),\n",
       " ('relish', 0.5677598118782043),\n",
       " ('ensembl', 0.5595935583114624),\n",
       " ('smallest', 0.5581124424934387),\n",
       " ('recogniz', 0.557616651058197),\n",
       " ('orlando', 0.5566136240959167),\n",
       " ('deniro', 0.5559999942779541),\n",
       " ('comedian', 0.5505030155181885),\n",
       " ('forb', 0.5500813722610474)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('actor',  topn = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6813883503689548"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('actor', \"actress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : vectors for words {'actrees'} are not present in the model, ignoring these words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'scene'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match('actor actrees scene'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stoltz\n"
     ]
    }
   ],
   "source": [
    "## As it was seen before, corpus is overloaded with names - semantic algebra here is not working like we might expect\n",
    "\n",
    "print(model.most_similar(positive = ['actor', 'actress'], negative = ['man'])[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine a model - CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "INFO : collecting all words and their counts\n",
      "INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO : PROGRESS: at sentence #10000, processed 2442598 words, keeping 33760 word types\n",
      "INFO : collected 37617 word types from a corpus of 3060552 raw words and 12500 sentences\n",
      "INFO : Loading a fresh vocabulary\n",
      "INFO : min_count=20 retains 7048 unique words (18% of original 37617, drops 30569)\n",
      "INFO : min_count=20 leaves 2945923 word corpus (96% of original 3060552, drops 114629)\n",
      "INFO : deleting the raw counts dictionary of 37617 items\n",
      "INFO : sample=0.001 downsamples 49 most-common words\n",
      "INFO : downsampling leaves estimated 2137631 word corpus (72.6% of prior 2945923)\n",
      "INFO : estimated required memory for 7048 words and 150 dimensions: 11981600 bytes\n",
      "INFO : resetting layer weights\n",
      "INFO : training model with 4 workers on 7048 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "INFO : PROGRESS: at 3.41% examples, 342113 words/s, in_qsize 8, out_qsize 3\n",
      "INFO : PROGRESS: at 6.99% examples, 353261 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 10.63% examples, 357045 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 14.24% examples, 358463 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 17.77% examples, 359534 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 21.35% examples, 360543 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 24.87% examples, 361070 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 28.50% examples, 361568 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 32.13% examples, 361917 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 35.71% examples, 361520 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 39.22% examples, 361176 words/s, in_qsize 7, out_qsize 1\n",
      "INFO : PROGRESS: at 42.85% examples, 361115 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 46.44% examples, 361088 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 50.05% examples, 361088 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 53.74% examples, 361056 words/s, in_qsize 5, out_qsize 2\n",
      "INFO : PROGRESS: at 57.23% examples, 360999 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 60.79% examples, 361047 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 64.40% examples, 361161 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 68.01% examples, 361189 words/s, in_qsize 7, out_qsize 1\n",
      "INFO : PROGRESS: at 71.69% examples, 361291 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 75.21% examples, 361411 words/s, in_qsize 8, out_qsize 2\n",
      "INFO : PROGRESS: at 78.73% examples, 361487 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 82.34% examples, 361579 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 85.87% examples, 361629 words/s, in_qsize 8, out_qsize 3\n",
      "INFO : PROGRESS: at 89.53% examples, 361651 words/s, in_qsize 8, out_qsize 3\n",
      "INFO : PROGRESS: at 93.20% examples, 361805 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : PROGRESS: at 96.71% examples, 361884 words/s, in_qsize 7, out_qsize 0\n",
      "INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "INFO : training on 15302760 raw words (10688329 effective words) took 29.5s, 362594 effective words/s\n",
      "INFO : saving Word2Vec object under ~/imdb/models/model_cbow.bin, separately None\n",
      "INFO : not storing attribute syn0norm\n",
      "INFO : not storing attribute cum_table\n",
      "INFO : saved ~/imdb/models/model_cbow.bin\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences,\n",
    "                    sg = 0,\n",
    "                    window = 10,\n",
    "                    min_count = 20,\n",
    "                    seed = 6,\n",
    "                    workers = 4,\n",
    "                    size = 150)\n",
    "model.save('~/imdb/models/model_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : loading Word2Vec object from ~/imdb/models/model_cbow.bin\n",
      "INFO : loading wv recursively from ~/imdb/models/model_cbow.bin.wv.* with mmap=None\n",
      "INFO : setting ignored attribute syn0norm to None\n",
      "INFO : setting ignored attribute cum_table to None\n",
      "INFO : loaded ~/imdb/models/model_cbow.bin\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cast\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = gensim.models.Word2Vec.load('~/imdb/models/model_cbow.bin')\n",
    "print(model.most_similar(positive = ['actor', 'actress'], negative = ['man'])[0][0])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
